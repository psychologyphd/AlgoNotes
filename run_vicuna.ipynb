{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psychologyphd/AlgoNotes/blob/main/run_vicuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Vicuna 13b on Colab\n",
        "\n",
        "by https://github.com/aaalgo/"
      ],
      "metadata": {
        "id": "6BlxSVBLuxOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPPOXJG7Pepr"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q bitsandbytes\n",
        "!pip3 install -q datasets sentencepiece\n",
        "!pip3 install -q git+https://github.com/huggingface/transformers\n",
        "!pip3 install -q accelerate\n",
        "!pip3 install -q safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install GPTQ from source code\n",
        "! cd /content && git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\n",
        "! cd /content/GPTQ-for-LLaMa && git checkout 9659310499cc7a0ea5498c1beb47bb228d65d178 && python3 setup_cuda.py build\n",
        "# Link .so file so python can find it\n",
        "! ln -s /content/GPTQ-for-LLaMa/build/lib*/*.so /content/GPTQ-for-LLaMa/"
      ],
      "metadata": {
        "id": "0QvH589smM03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download vicuna model, the file is 7G so it will take a while\n",
        "! cd /content && git lfs install && git clone https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\n"
      ],
      "metadata": {
        "id": "OGxAZK0dmd4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -lh /content/vicuna-13b-GPTQ-4bit-128g"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhjrkX4bUrK8",
        "outputId": "631758a6-9aa2-4c3c-a21e-3591baeffea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 7.0G\n",
            "-rw-r--r-- 1 root root  507 Apr 10 14:39 config.json\n",
            "-rw-r--r-- 1 root root  137 Apr 10 14:39 generation_config.json\n",
            "-rw-r--r-- 1 root root  33K Apr 10 14:39 pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  606 Apr 10 14:39 README.md\n",
            "-rw-r--r-- 1 root root  411 Apr 10 14:39 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  727 Apr 10 14:39 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 489K Apr 10 14:39 tokenizer.model\n",
            "-rw-r--r-- 1 root root 7.0G Apr 10 14:42 vicuna-13b-4bit-128g.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys \n",
        "from pathlib import Path\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import accelerate\n",
        "sys.path.insert(0, \"/content/GPTQ-for-LLaMa\")\n",
        "\n",
        "import llama_inference_offload\n",
        "from modelutils import find_layers\n",
        "from quant import make_quant\n",
        "\n",
        "\n",
        "def _load_quant(model, checkpoint, wbits, groupsize=-1, faster_kernel=False, exclude_layers=['lm_head'], kernel_switch_threshold=128):\n",
        "    config = AutoConfig.from_pretrained(model)\n",
        "    def noop(*args, **kwargs):\n",
        "        pass\n",
        "    torch.nn.init.kaiming_uniform_ = noop \n",
        "    torch.nn.init.uniform_ = noop \n",
        "    torch.nn.init.normal_ = noop \n",
        "\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    transformers.modeling_utils._init_weights = False\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "    torch.set_default_dtype(torch.float)\n",
        "    model = model.eval()\n",
        "    layers = find_layers(model)\n",
        "    for name in exclude_layers:\n",
        "        if name in layers:\n",
        "            del layers[name]\n",
        "    make_quant(model, layers, wbits, groupsize)\n",
        "    del layers\n",
        "    \n",
        "    print('Loading model ...')\n",
        "    from safetensors.torch import load_file as safe_load\n",
        "    model.load_state_dict(safe_load(checkpoint))\n",
        "    model.seqlen = 1024\n",
        "    print('Done.')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BMW1icovl3Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/vicuna-13b-GPTQ-4bit-128g'\n",
        "pt_path = '/content/vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors'\n",
        "\n",
        "model = _load_quant(model_path, pt_path, 4, 128)\n",
        "model = model.to(torch.device('cuda:0'))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)"
      ],
      "metadata": {
        "id": "1DhrxcdyQMN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def inference (instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())\n",
        "  "
      ],
      "metadata": {
        "id": "dILk1YFaRZ1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = 'How to make a sandwich?' #@param {type:\"string\"}\n",
        "input = '' #@param {type: \"string\"}\n",
        "\n",
        "inference(instruction, input)"
      ],
      "metadata": {
        "id": "tKRQV-L0RKZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6395d0b9-b11c-4063-b74c-0d418f11c448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: To make a sandwich, you will need:\n",
            "\n",
            "* 2 slices of bread\n",
            "* Butter or mayonnaise (optional)\n",
            "* Sliced meats (such as ham, turkey, or chicken)\n",
            "* Sliced cheese (such as cheddar or swiss)\n",
            "* Lettuce, tomato, and/or onion (optional)\n",
            "* Any additional toppings of your choice (such as pickles, avocado, or mustard)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Lay one slice of bread on a clean surface.\n",
            "2. Spread a thin layer of butter or mayonnaise on one side of the bread (optional).\n",
            "3. On the other side of the bread, layer on the sliced meats and cheese.\n",
            "4. Add any additional toppings that you desire.\n",
            "5. Top with the other slice of bread, with the buttered or mayonnaised side facing up.\n",
            "6. Gently press down on the top slice of bread to help the sandwich hold together.\n",
            "7. Cut the sandwich in half or in diagonal slices, and serve.\n",
            "### Assistant:\n"
          ]
        }
      ]
    }
  ]
}